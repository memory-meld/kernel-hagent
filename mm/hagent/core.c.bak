// SPDX-License-Identifier: GPL-2.0
/*
 * Copyright (C) 2021-2024 Junliang Hu
 *
 * Author: Junliang Hu <jlhu@cse.cuhk.edu.hk>
 *
 */

#include <linux/sched/clock.h>
#include <linux/mm.h>
#include <linux/mm_inline.h>
#include <linux/slab.h>
#include <linux/swap.h>
#include <linux/mempolicy.h>

#include "mpsc.h"
#include "sds.h"
#include "indexable_heap.h"
#include "module.h"

#define TRY(exp, ...)                                 \
	({                                            \
		__auto_type __val = ((exp));          \
		long err = (long)(void *)(long)__val; \
		if (IS_ERR_VALUE(err)) {              \
			pr_err(__VA_ARGS__);          \
			return err;                   \
		}                                     \
		__val;                                \
	})

enum {
	DIRECTION_DEMOTION,
	DIRECTION_PROMOTION,
	DIRECTION_MAX,
};

enum {
	THREAD_POLICY,
	THREAD_MIGRATION,
	THREAD_MAX,
};

// CWISS_DECLARE_FLAT_HASHMAP(HashMapU64Ptr, u64, void *);

enum {
	EXEC_TIME_TOTAL,
	EXEC_TIME_OVERFLOW_HANDLER,
	EXEC_TIME_POLICY,
	EXEC_TIME_MIGRATION,
	EXEC_TIME_SINCE_THROTTLE,
	EXEC_TIME_SINCE_THROTTLE_POLICY,
	EXEC_TIME_MPSC,
	EXEC_TIME_SDS,
	EXEC_TIME_INDEXABLE_HEAP,
	EXEC_TIME_EXCHANGE,
	EXEC_TIME_PERF_PREPARE,
	EXEC_TIME_MAX,
};

// Represent a hagent management target, i.e. a process.
struct hagent_target {
	struct task_struct *task;
	mpsc_t chan;
	union {
		struct task_struct *threads[THREAD_MAX];
		struct delayed_work works[THREAD_MAX];
	};
	u64 start_time;
	atomic_t enabled;
	atomic_long_t exec_time[EXEC_TIME_MAX];
	struct hrtimer timer;
	wait_queue_head_t timer_wait;
	atomic_long_t tick;
	struct delayed_work stop;
	struct completion stopped;
	struct perf_event *events[MAX_EVENTS];
	struct mutex lock;
	// All managed struct folio *
	struct list_head managed;
	// vpfn -> struct folio *
	HashMapU64Ptr map;
	struct sds sds;
	struct indexable_heap heap[DIRECTION_MAX];
	// struct spsc decayed;
	u64 total_samples, dram_samples, pmem_samples, next_migration,
		next_dump, total_exchanges, next_report;
};

DECLARE_WAIT_QUEUE_HEAD(hagent_wait);

DEFINE_LOCK_GUARD_1(mmap_read_lock, struct mm_struct, mmap_read_lock(_T->lock),
		    mmap_read_unlock(_T->lock));
extern void perf_prepare_sample(struct perf_sample_data *data,
				struct perf_event *event, struct pt_regs *regs);
extern int folio_exchange(struct folio *old, struct folio *new,
			  enum migrate_mode mode);
extern int folio_exchange_isolated(struct folio *old, struct folio *new,
				   enum migrate_mode mode);

extern void resolve_folio_cleanup(struct folio **foliop);

struct hagent_target_work_time_guard {
	struct hagent_target *self;
	u64 start;
	u64 item;
};
DEFINE_CLASS(hagent_target_work_time_guard,
	     struct hagent_target_work_time_guard, ({
		     atomic_long_add(sched_clock() - _T.start,
				     &_T.self->exec_time[_T.item]);
	     }),
	     ({
		     struct hagent_target_work_time_guard __t = {
			     .self = self,
			     .item = item,
			     .start = sched_clock(),
		     };
		     __t;
	     }),
	     struct hagent_target *self, u64 item);

static char const *exec_time_item_name[] = {
	"total",	  "overflow_handler",
	"policy_work",	  "migration_work",
	"since_throttle", "since_throttle_policy_work",
	"mpsc",		  "sds",
	"indexable_heap", "exchange",
	"perf_prepare",
};

noinline static inline void
hagent_target_event_overflow(struct perf_event *event,
			     struct perf_sample_data *data,
			     struct pt_regs *regs)
{
	struct hagent_target *self = event->overflow_handler_context;
	bool batch_last = regs->cx == 0;
	CLASS(hagent_target_work_time_guard, __t1)
	(self, EXEC_TIME_OVERFLOW_HANDLER);

	// for locking see: __perf_event_output
	scoped_guard(rcu) scoped_guard(irqsave)
	{
		{
			CLASS(hagent_target_work_time_guard, __tch)
			(self, EXEC_TIME_PERF_PREPARE);
			perf_prepare_sample(data, event, regs);
		}
		struct perf_sample s = {
			.config = event->attr.config,
			.config1 = event->attr.config1,
			.pid = data->tid_entry.pid,
			.tid = data->tid_entry.tid,
			.time = data->time,
			.addr = data->addr,
			.weight = data->weight.full,
			.phys_addr = data->phys_addr,
		};

		CLASS(hagent_target_work_time_guard, __tch)
		(self, EXEC_TIME_MPSC);
		if (mpsc_send(self->chan, &s, sizeof(s)) < 0) {
			// This should never happen as we created the mpsc using
			// overwritting mode.
			pr_warn_ratelimited(
				"%s: discard sample due to ring buffer overflow\n",
				__func__);
		};
	}
	if (static_branch_likely(&use_asynchronous_architecture) &&
	    batch_last) {
		queue_delayed_work(system_unbound_wq,
				   &self->works[THREAD_POLICY], 1);
	}
}
static void hagnet_target_events_release(struct hagent_target *self)
{
	for (int i = 0; i < MAX_EVENTS; ++i) {
		if (self->events[i])
			perf_event_disable(self->events[i]);
	}
	for (int i = 0; i < MAX_EVENTS; ++i) {
		if (self->events[i])
			perf_event_release_kernel(self->events[i]);
	}
}
static int hagnet_target_events_create(struct hagent_target *self,
				       struct task_struct *task)
{
	for (int i = 0; i < MAX_EVENTS; ++i) {
		struct perf_event *e = perf_event_create_kernel_counter(
			&event_attrs[i], -1, task, hagent_target_event_overflow,
			self);
		if (IS_ERR(e)) {
			hagnet_target_events_release(self);
			return PTR_ERR(e);
		}
		self->events[i] = e;

		pr_info("%s: created config=0x%llx sample_period=%lld\n",
			__func__, event_attrs[i].config,
			event_attrs[i].sample_period);
	}
	return 0;
}
static struct folio *uvirt_to_folio(struct mm_struct *mm, u64 user_addr)
{
	guard(mmap_read_lock)(mm);
	struct vm_area_struct *vma = vma_lookup(mm, user_addr);
	if (!vma)
		return ERR_PTR(-EFAULT);
	// if (vma->vm_flags & VM_MAYEXEC) {
	// 	// Ignore exectuable pages
	// 	return ERR_PTR(-EFAULT);
	// }
	struct page *page = follow_page(vma, user_addr, FOLL_GET | FOLL_DUMP);
	if (IS_ERR_OR_NULL(page))
		return ERR_CAST(page);
	return page_folio(page);
}

static void cleanup_mmput(struct mm_struct **mm)
{
	if (!mm || IS_ERR_OR_NULL(*mm))
		return;
	mmput(*mm);
}

static bool hagent_target_managed(struct hagent_target *self, u64 vpfn)
{
	return HashMapU64Ptr_contains(&self->map, &vpfn);
}
static struct folio *hagent_target_managed_folio(struct hagent_target *self,
						 u64 vpfn)
{
	HashMapU64Ptr_Iter iter = HashMapU64Ptr_find(&self->map, &vpfn);
	HashMapU64Ptr_Entry *entry = HashMapU64Ptr_Iter_get(&iter);
	return entry ? entry->val : NULL;
}
extern bool folio_isolate_lru(struct folio *folio);
extern void folio_putback_lru(struct folio *folio);
DEFINE_CLASS(folio_get, struct folio *, folio_put(_T), ({
		     folio_get(folio);
		     folio;
	     }),
	     struct folio *folio);
static int hagent_target_manage(struct hagent_target *self, u64 vpfn,
				struct folio *folio)
{
	// if (hagent_target_managed(self, vpfn))
	// 	return -EEXIST;

	CLASS(folio_get, __folio)(folio);
	if (!folio_isolate_lru(folio))
		return -EAGAIN;
	/*
	 * Isolating the folio has taken another reference, so the
	 * caller's reference can be safely dropped without the folio
	 * disappearing underneath us during migration.
	 */
	// folio_put(folio);
	list_add_tail(&folio->lru, &self->managed);
	HashMapU64Ptr_Entry const entry = { vpfn, folio };
	HashMapU64Ptr_insert(&self->map, &entry);
	node_stat_mod_folio(folio, NR_ISOLATED_ANON + folio_is_file_lru(folio),
			    folio_nr_pages(folio));
	return 0;
}
static int hagent_target_unmanage(struct hagent_target *self,
				  struct folio *folio, u64 vpfn)
{
	if (!hagent_target_managed(self, vpfn))
		return -ENOENT;

	HashMapU64Ptr_erase(&self->map, &vpfn);
	list_del(&folio->lru);
	node_stat_mod_folio(folio, NR_ISOLATED_ANON + folio_is_file_lru(folio),
			    -folio_nr_pages(folio));
	folio_putback_lru(folio);
	// folio_add_lru(folio);
	return 0;
}
static int hagent_target_unmanage_all(struct hagent_target *self)
{
	struct folio *folio, *next;
	size_t size = HashMapU64Ptr_size(&self->map);
	pr_info("%s: size=%zu\n", __func__, size);
	list_for_each_entry_safe(folio, next, &self->managed, lru) {
		list_del(&folio->lru);
		node_stat_mod_folio(folio,
				    NR_ISOLATED_ANON + folio_is_file_lru(folio),
				    -folio_nr_pages(folio));
		folio_putback_lru(folio);
		// folio_add_lru(folio);
	}
	BUG_ON(!list_empty(&folio->lru));
	HashMapU64Ptr_clear(&self->map);
	return 0;
}

static enum hrtimer_restart hagent_target_timer_fn(struct hrtimer *timer)
{
	struct hagent_target *self =
		container_of(timer, struct hagent_target, timer);

	return HRTIMER_RESTART;
}

static bool hagent_target_event_check_enable(struct hagent_target *self)
{
	u64 now = sched_clock(), elapsed = now - self->start_time;
	u64 secs = elapsed / 1000000000ul, period = HAGENT_ENABLE_DURATION,
	    cycle = secs / period, mod = secs % period;
	if (mod < HAGENT_ENABLE_DURATION) {
		// Should be enabled
		if (atomic_cmpxchg(&self->enabled, false, true) == false) {
			pr_info_ratelimited(
				"%s: enable=%d disable=%d secs=%llu period=%llu cycle=%llu mod=%lld enabling\n",
				__func__, HAGENT_ENABLE_DURATION,
				HAGENT_DISABLE_DURATION, secs, period, cycle,
				mod);
			for (int i = 0; i < MAX_EVENTS; ++i)
				perf_event_enable(self->events[i]);
		}
	} else {
		// Should be disabled
		if (atomic_cmpxchg(&self->enabled, true, false) == true) {
			pr_info_ratelimited(
				"%s: enable=%d disable=%d secs=%llu period=%llu cycle=%llu mod=%lld disabling\n",
				__func__, HAGENT_ENABLE_DURATION,
				HAGENT_DISABLE_DURATION, secs, period, cycle,
				mod);
			for (int i = 0; i < MAX_EVENTS; ++i)
				perf_event_disable(self->events[i]);
		}
	}
	return true;
}
noinline static void hagent_target_event_throttle(struct hagent_target *self)
{
	// if (!hagent_target_event_check_enable(self))
	// 	return;
	if (!static_branch_likely(&should_throttle_event_period))
		return;
	u64 now = sched_clock(),
	    elapsed = now - atomic_long_read(
				    &self->exec_time[EXEC_TIME_SINCE_THROTTLE]),
	    upper_limit = desired_cpu_utilization + EPISLON_CPU_UTILIZATION,
	    lower_limit = desired_cpu_utilization - EPISLON_CPU_UTILIZATION;
	// Wait for at least 10 seconds before deciding to throttle or not
	if (elapsed < THROTTLE_PERIOD)
		return;
	bool should_throttle = false, should_unthrottle = false;
	u64 throttle_ratio = MIN_THROTTLE_RATIO,
	    unthrottle_ratio = MAX_THROTTLE_RATIO;
	u64 throttle_items[] = {
		EXEC_TIME_SINCE_THROTTLE_POLICY,
	};
	for (int i = 0; i < ARRAY_SIZE(throttle_items) && !should_throttle &&
			!should_unthrottle;
	     ++i) {
		u64 item = throttle_items[i];
		u64 exec_time = atomic_long_read(&self->exec_time[item]);
		u64 usage = exec_time * 100 / elapsed;
		if (usage > upper_limit) {
			// Do not need to check division by zero because elapsed
			// is always not less than THROTTLE_PERIOD
			should_throttle = true;
			// Make throttle a little conservative
			throttle_ratio =
				max(exec_time * 100 /
					    (desired_cpu_utilization * elapsed),
				    throttle_ratio);
			pr_info_ratelimited(
				"%s: throttle work=%d exec_time=%llu elapsed=%llu usage=%llu upper_limit=%llu throttle_ratio=%llu\n",
				__func__, i, exec_time, elapsed, usage,
				upper_limit, throttle_ratio);
			break;
		}
		if (usage < lower_limit) {
			should_unthrottle = true;
			// Make unthrottle a little aggressive
			unthrottle_ratio =
				min(DIV_ROUND_UP_ULL(desired_cpu_utilization *
							     elapsed,
						     exec_time * 100 + 1),
				    unthrottle_ratio);
			pr_info_ratelimited(
				"%s: unthrottle work=%d exec_time=%llu elapsed=%llu usage=%llu lower_limit=%llu unthrottle_ratio=%llu\n",
				__func__, i, exec_time, elapsed, usage,
				lower_limit, unthrottle_ratio);
			break;
		}
	}
	if (should_throttle) {
		for (int i = 0; i < MAX_EVENTS; ++i) {
			struct perf_event_attr *attr = &event_attrs[i];
			attr->sample_period =
				min(attr->sample_period * throttle_ratio,
				    SAMPLE_PERIOD_MAX);
			perf_event_disable(self->events[i]);
			perf_event_period(self->events[i], attr->sample_period);
			perf_event_enable(self->events[i]);
			pr_info_ratelimited(
				"%s: throttle event=%d config=0x%llx sample_period=%lld\n",
				__func__, i, attr->config, attr->sample_period);
		}
		for (int i = 0; i < ARRAY_SIZE(throttle_items); ++i) {
			u64 item = throttle_items[i];
			atomic_long_set(&self->exec_time[item], 0);
		}
		atomic_long_set(&self->exec_time[EXEC_TIME_SINCE_THROTTLE], 0);
	}
	if (should_unthrottle) {
		for (int i = 0; i < MAX_EVENTS; ++i) {
			struct perf_event_attr *attr = &event_attrs[i];
			// unthrottle_ratio is always no less than THROTTLE_RATIO
			attr->sample_period =
				max(attr->sample_period / unthrottle_ratio,
				    SAMPLE_PERIOD_MIN);
			perf_event_disable(self->events[i]);
			perf_event_period(self->events[i], attr->sample_period);
			perf_event_enable(self->events[i]);
			pr_info_ratelimited(
				"%s: unthrottle event=%d config=0x%llx sample_period=%lld\n",
				__func__, i, attr->config, attr->sample_period);
		}
		for (int i = 0; i < ARRAY_SIZE(throttle_items); ++i) {
			u64 item = throttle_items[i];
			atomic_long_set(&self->exec_time[item], 0);
		}
		atomic_long_set(&self->exec_time[EXEC_TIME_SINCE_THROTTLE], 0);
	}
}

noinline static void hagent_target_work_fn_policy(struct work_struct *work)
{
	struct hagent_target *self =
		container_of(work, typeof(*self), works[THREAD_POLICY].work);
	CLASS(hagent_target_work_time_guard, __t1)
	(self, EXEC_TIME_POLICY);
	CLASS(hagent_target_work_time_guard, __t2)
	(self, EXEC_TIME_SINCE_THROTTLE_POLICY);

	struct mm_struct *mm __cleanup(cleanup_mmput) = get_task_mm(self->task);
	if (IS_ERR_OR_NULL(mm)) {
		pr_err("%s: get_task_mm()=%pe failed\n", __func__, mm);
		return;
	}
	// FIXME: implement additional vmstat counters
	// guard(vmevent)(HOTNESS_IDENTIFICATION_COST);
	scoped_cond_guard(mutex_try, ({
				  // count_vm_event(HAGENT_LOCK_FAILED);
				  count_vm_event(HAGENT_LOCK_FAILED_POLICY);
				  return;
			  }),
			  &self->lock)
	{
		// count_vm_event(HAGENT_LOCK_ACQUIRED);
		count_vm_event(HAGENT_LOCK_ACQUIRED_POLICY);
		hagent_target_event_throttle(self);
		u64 total_samples = 0, dram_samples = 0, pmem_samples = 0;
		struct perf_sample s;
		// Make two pass over each CPU's send buffer
		for (u64 failure = 0, retry = 2 * num_online_cpus(),
			 cpu = cpumask_first(cpu_online_mask);
		     failure < retry;) {
			{
				CLASS(hagent_target_work_time_guard, __tch)
				(self, EXEC_TIME_MPSC);
				if (sizeof(s) != mpsc_recv_cpu(self->chan, cpu,
							       &s, sizeof(s))) {
					++failure;
					cpu = cpumask_next(cpu,
							   cpu_online_mask);
					if (cpu >= nr_cpu_ids) {
						cpu = cpumask_first(
							cpu_online_mask);
					}
					continue;
				}
			}
			u64 vpfn = s.addr >> PAGE_SHIFT;
			if (s.pid != self->task->tgid || !vpfn) {
				count_vm_event(PEBS_NR_DISCARDED);
				// count_vm_event(PEBS_NR_DISCARDED_INCOMPLETE);
				continue;
			}
			total_samples++;
			struct folio *folio __cleanup(resolve_folio_cleanup) =
				uvirt_to_folio(mm, s.addr);
			if (IS_ERR_OR_NULL(folio) || folio_mapping(folio)) {
				count_vm_event(PEBS_NR_DISCARDED);
				// count_vm_event(PEBS_NR_DISCARDED_FILE);
				// Ignore file pages for now
				continue;
			}
			bool managed = hagent_target_managed(self, vpfn);
			if (!folio_test_lru(folio) && !managed) {
				count_vm_event(PEBS_NR_DISCARDED);
				// count_vm_event(PEBS_NR_DISCARDED_NONLRU);
				continue;
			}
			if (!managed &&
			    hagent_target_manage(self, vpfn, folio)) {
				count_vm_event(PEBS_NR_DISCARDED);
				// count_vm_event(PEBS_NR_DISCARDED_ISOLATE);
				continue;
			}
			bool in_dram = folio_nid(folio) == DRAM_NID;
			in_dram ? dram_samples++ : pmem_samples++;
			u64 count = ({
				CLASS(hagent_target_work_time_guard, __tsds)
				(self, EXEC_TIME_SDS);
				sds_push_multiple(
					&self->sds, vpfn,
					s.config == MEM_TRANS_RETIRED_LOAD_LATENCY ?
						10 :
						1);
			});
			CLASS(hagent_target_work_time_guard, __tih)

			(self, EXEC_TIME_INDEXABLE_HEAP);
			struct indexable_heap *heap =
				&self->heap[in_dram ? DIRECTION_DEMOTION :
						      DIRECTION_PROMOTION];
			if (indexable_heap_contains(heap, vpfn)) {
				if (count) {
					indexable_heap_update(heap, vpfn,
							      count);
					// pr_info_ratelimited(
					// 	"%s: updated vpfn=%llu count=%llu\n",
					// 	__func__, vpfn, count);
				} else {
					// DRAM folios without access means they
					// are cold and are great candidates for
					// demotion
					if (!in_dram)
						indexable_heap_erase(heap,
								     vpfn);
				}
			} else {
				if (count) {
					indexable_heap_push(heap, vpfn, count);
					// pr_info_ratelimited(
					// 	"%s: pushed vpfn=%llu count=%llu\n",
					// 	__func__, vpfn, count);
				} else {
					// ignore
				}
			}
		}
		self->total_samples += total_samples;
		self->dram_samples += dram_samples;
		self->pmem_samples += pmem_samples;
		count_vm_events(PEBS_NR_SAMPLED, total_samples);
		count_vm_events(PEBS_NR_SAMPLED_FMEM, dram_samples);
		count_vm_events(PEBS_NR_SAMPLED_SMEM, pmem_samples);
	}
	if (self->total_samples > self->next_migration) {
		self->next_migration += 10000;
		pr_info("%s: collected samples dram=%llu pmem=%llu managed=%zu dheap=%zu pheap=%zu\n",
			__func__, self->dram_samples, self->pmem_samples,
			HashMapU64Ptr_size(&self->map),
			indexable_heap_size(&self->heap[DIRECTION_DEMOTION]),
			indexable_heap_size(&self->heap[DIRECTION_PROMOTION]));
		queue_delayed_work(system_highpri_wq,
				   &self->works[THREAD_MIGRATION], 0);
	}
}
noinline static void hagent_target_work_fn_migration(struct work_struct *work)
{
	struct hagent_target *self =
		container_of(work, typeof(*self), works[THREAD_MIGRATION].work);
	CLASS(hagent_target_work_time_guard, __t1)
	(self, EXEC_TIME_MIGRATION);

	lru_add_drain_all();

	struct mm_struct *mm __cleanup(cleanup_mmput) = get_task_mm(self->task);
	if (IS_ERR_OR_NULL(mm)) {
		pr_err("%s: get_task_mm()=%pe failed\n", __func__, mm);
		return;
	}
	// FIXME: implement additional vmstat counters
	// guard(vmevent)(PAGE_MIGRATION_COST);
	scoped_cond_guard(mutex_try, ({
				  // count_vm_event(HAGENT_LOCK_FAILED);
				  count_vm_event(HAGENT_LOCK_FAILED_MIGRATION);
				  return;
			  }),
			  &self->lock)
	{
		// count_vm_event(HAGENT_LOCK_ACQUIRED);
		count_vm_event(HAGENT_LOCK_ACQUIRED_MIGRATION);
		u64 found_pair = 0, total_exchanges = 0, failed = 0;
		struct indexable_heap *dheap = &self->heap[DIRECTION_DEMOTION],
				      *pheap = &self->heap[DIRECTION_PROMOTION];
		if (self->total_samples > self->next_dump) {
			self->next_dump += 100000;
			// indexable_heap_dump(dram_heap, 1000);
			// indexable_heap_dump(pmem_heap, 1000);
		}

		while (true) {
			if (indexable_heap_size(dheap) < 512) {
				// Refill
				// pr_info("%s: ===== FINISHED: nothing to demotion =====",
				// 	__func__);
				break;
			}
			if (indexable_heap_size(pheap) < 512) {
				// We don't need to refill the promotion heap manually,
				// when we receive a sample, we will update the heap
				// pr_info("%s: ===== FINISHED: nothing to promotion =====",
				// 	__func__);
				break;
			}
			u64 dram_vpfn = *kv_ckey(indexable_heap_peek(dheap)),
			    pmem_vpfn = *kv_ckey(indexable_heap_peek(pheap));
			u64 dram_count = *kv_cvalue(indexable_heap_peek(dheap)),
			    pmem_count = *kv_cvalue(indexable_heap_peek(pheap));
			if (dram_count >= pmem_count) {
				// DRAM should contain the hottest data
				// pr_info("%s: ===== FINISHED: no misplaced =====",
				// __func__);
				break;
			}
			// struct folio *src __cleanup(resolve_folio_cleanup) =
			// 	uvirt_to_folio(mm, dram_vpfn << PAGE_SHIFT);
			struct folio *src =
				hagent_target_managed_folio(self, dram_vpfn);

			if (IS_ERR_OR_NULL(src) || folio_nid(src) != DRAM_NID) {
				// Remove invalid pfn from the heap to avoid
				// dead loop
				CLASS(hagent_target_work_time_guard, __tih)
				(self, EXEC_TIME_INDEXABLE_HEAP);
				indexable_heap_pop(dheap);
				continue;
			}

			// struct folio *dst __cleanup(resolve_folio_cleanup) =
			// 	uvirt_to_folio(mm, pmem_vpfn << PAGE_SHIFT);
			struct folio *dst =
				hagent_target_managed_folio(self, pmem_vpfn);
			if (IS_ERR_OR_NULL(dst) || folio_nid(dst) == DRAM_NID) {
				// Remove invalid pfn from the heap to avoid
				// dead loop
				CLASS(hagent_target_work_time_guard, __tih)
				(self, EXEC_TIME_INDEXABLE_HEAP);
				indexable_heap_pop(pheap);
				continue;
			}

			{
				CLASS(hagent_target_work_time_guard, __tih)
				(self, EXEC_TIME_INDEXABLE_HEAP);
				indexable_heap_pop(dheap);
				indexable_heap_pop(pheap);
			}
			++found_pair;

			// pr_info("%s: pid=%d task=%p usage=%d", __func__,
			// 	self->task->tgid, self->task,
			// 	self->task->usage);
			// pr_info("%s: ===== EXCHANGE EXECUTING ====", __func__);
			// long err = folio_exchange(src, dst, MIGRATE_SYNC);

			// resolve folio has taken another reference, exceeding
			// the expected reference count during exchange
			long err = ({
				CLASS(hagent_target_work_time_guard, __tih)
				(self, EXEC_TIME_EXCHANGE);
				folio_exchange_isolated(src, dst, MIGRATE_SYNC);
			});
			if (err) {
				++failed;
				// clang-format off
				pr_err("%s: exchange_folio: mode=%d err=%pe [src=%p vaddr=%p pfn=0x%lx] <-> [dst=%p vaddr=%p pfn=0x%lx]",
				       __func__, MIGRATE_SYNC, ERR_PTR(err),
				       src, (void *)(dram_vpfn << PAGE_SHIFT), folio_pfn(src),
				       dst, (void *)(pmem_vpfn << PAGE_SHIFT), folio_pfn(dst));
				// clang-format on
				// FIXME: should we put back the folio to the heap so that we can try at an another time?
			} else {
				CLASS(hagent_target_work_time_guard, __tih)
				(self, EXEC_TIME_INDEXABLE_HEAP);
				// Althoug in theory the folio could only be
				// backed by DRAM or PMEM, in practive, the
				// kernel could have done migration in the
				// background that we are not aware of.
				if (indexable_heap_contains(dheap, pmem_vpfn)) {
					indexable_heap_update(dheap, pmem_vpfn,
							      dram_count);
				} else {
					indexable_heap_push(dheap, pmem_vpfn,
							    dram_count);
				}
				if (indexable_heap_contains(pheap, dram_vpfn)) {
					indexable_heap_update(pheap, dram_vpfn,
							      pmem_count);
				} else {
					indexable_heap_push(pheap, dram_vpfn,
							    pmem_count);
				}
				total_exchanges++;
				// FIXME: implement additional vmstat counters
				// count_vm_event(PAGE_PROMOTED);
				// count_vm_event(PAGE_DEMOTED);
				// count_vm_event(PAGE_EXCHANGED);
			}
			// pr_info("%s: ===== EXCHANGE RETURNED ====", __func__);

			if (found_pair >= migration_batch_size ||
			    failed * 10 >= migration_batch_size) {
				pr_info_ratelimited(
					"%s: found pair=%llu failed=%llu\n",
					__func__, found_pair, failed);
				break;
			}
			// pr_info("%s: next iteration\n", __func__);
		}
		self->total_exchanges += total_exchanges;
		if (self->total_exchanges > self->next_report) {
			self->next_report += 512 * 50;
			pr_info("%s: total exchanges=%llu\n", __func__,
				self->total_exchanges);
		}
	}
	// pr_info("%s: returned\n", __func__);
}

static void hagent_target_work_drop(struct hagent_target *self);
static void hagent_target_work_fn_stop(struct work_struct *work)
{
	struct hagent_target *self =
		container_of(work, typeof(*self), stop.work);
	hrtimer_cancel(&self->timer);
	u64 total_elapsed = sched_clock() - self->start_time;
	pr_info("%s: total_elapsed=%llu\n", __func__, total_elapsed);
	for (int i = 0; i < EXEC_TIME_MAX; ++i) {
		long val = atomic_long_read(&self->exec_time[i]);
		pr_info("%s: %s=%ld permyriad=%llu\n", __func__,
			exec_time_item_name[i], val,
			val * 10000 / total_elapsed);
	}
	hagnet_target_events_release(self);
	hagent_target_work_drop(self);
	complete(&self->stopped);
}

static void (*hagent_target_work_fn[THREAD_MAX])(struct work_struct *) = {
	[THREAD_POLICY] = hagent_target_work_fn_policy,
	[THREAD_MIGRATION] = hagent_target_work_fn_migration,
};
static char *hagent_target_work_name[THREAD_MAX] = {
	[THREAD_POLICY] = "hagent_target_work_policy",
	[THREAD_MIGRATION] = "hagent_target_work_migration",
};
static void hagent_target_work_drop(struct hagent_target *self)
{
	for (int i = 0; i < THREAD_MAX; ++i) {
		pr_info("%s: cancel_delayed_work_sync(%s)\n", __func__,
			hagent_target_work_name[i]);
		// Scynchrously stop the work and wait for exit
		cancel_delayed_work_sync(&self->works[i]);
	}
}
static int hagent_target_work_init(struct hagent_target *self)
{
	INIT_DELAYED_WORK(&self->stop, hagent_target_work_fn_stop);
	init_completion(&self->stopped);
	for (int i = 0; i < THREAD_MAX; ++i) {
		INIT_DELAYED_WORK(&self->works[i], hagent_target_work_fn[i]);
		pr_info("%s: INIT_DELAYED_WORK(%s)\n", __func__,
			hagent_target_work_name[i]);
	}
	return 0;
}

void hagent_target_drop(struct hagent_target *self)
{
	pr_info("%s: pid=%d", __func__, self->task->pid);
	queue_delayed_work(system_freezable_wq, &self->stop, 0);
	wait_for_completion(&self->stopped);
	for (int i = 0; i < DIRECTION_MAX; ++i)
		indexable_heap_drop(&self->heap[i]);
	sds_drop(&self->sds);
	// We have to destroy as a whole as we do not know the vpfn
	hagent_target_unmanage_all(self);
	HashMapU64Ptr_destroy(&self->map);
	if (self->chan)
		mpsc_drop(self->chan);
	if (self->task)
		put_task_struct(self->task);
	kfree(self);
}
struct hagent_target *hagent_target_new(pid_t pid)
{
	pr_info("%s: pid=%d", __func__, pid);
	struct hagent_target *self = kzalloc(sizeof(struct hagent_target),
					     GFP_KERNEL | __GFP_NOWARN);
	if (!self)
		return ERR_PTR(-ENOMEM);

	struct task_struct *task = find_get_task_by_vpid(pid);
	if (!task) {
		hagent_target_drop(self);
		return ERR_PTR(-ESRCH);
	}
	self->task = task;
	pr_info("%s: pid=%d task=%p usage=%d", __func__, pid, task,
		task->usage);

	self->start_time = sched_clock();

	mpsc_t chan = mpsc_new(sizeof(struct perf_sample) * CHANNEL_NELEMS);
	if (!chan) {
		hagent_target_drop(self);
		return ERR_PTR(-ENOMEM);
	}
	self->chan = chan;

	mutex_init(&self->lock);

	INIT_LIST_HEAD(&self->managed);
	self->map = HashMapU64Ptr_new(32);

	int err = sds_init_default(&self->sds);
	if (err) {
		hagent_target_drop(self);
		return ERR_PTR(err);
	}

	for (int i = 0; i < DIRECTION_MAX; ++i) {
		int err = indexable_heap_init(
			&self->heap[i], i == DIRECTION_DEMOTION,
			i == DIRECTION_DEMOTION ? "dram" : "pmem");
		if (err) {
			hagent_target_drop(self);
			return ERR_PTR(err);
		}
	}

	if (asynchronous_architecture) {
		static_branch_enable(&use_asynchronous_architecture);
		pr_info("%s: use asynchronous architecture\n", __func__);
		int err = hagent_target_work_init(self);
		if (err) {
			hagent_target_drop(self);
			return ERR_PTR(err);
		}
	} else {
		static_branch_disable(&use_asynchronous_architecture);
		pr_info("%s: use threaded architecture\n", __func__);
		pr_err("%s: threaded architecture is not implemented\n",
		       __func__);
		BUG();
	}

	if (decay_sketch) {
		static_branch_enable(&should_decay_sketch);
		pr_info("%s: decay histogram\n", __func__);
	} else {
		static_branch_disable(&should_decay_sketch);
		pr_info("%s: do not decay histogram\n", __func__);
	}

	if (throttle_event_period) {
		static_branch_enable(&should_throttle_event_period);
		pr_info("%s: throttle event period\n", __func__);
	} else {
		static_branch_disable(&should_throttle_event_period);
		pr_info("%s: do not throttle event period\n", __func__);
	}

	err = hagnet_target_events_create(self, task);
	if (err) {
		hagent_target_drop(self);
		return ERR_PTR(err);
	}
	atomic_set(&self->enabled, 1);
	hrtimer_init(&self->timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
	self->timer.function = hagent_target_timer_fn;
	hrtimer_start(&self->timer, ms_to_ktime(1000), HRTIMER_MODE_REL);

	pr_info("%s: pid=%d", __func__, pid);
	return self;
}

pid_t hagent_target_pid(struct hagent_target *self)
{
	return self->task->pid;
}

// static int policy_process_samples_locked(struct hagent_target *self,
// 					 struct mm_struct *mm)
// {
// 	u64 found_pair = 0, total_exchanges = 0, failed = 0;
// 	struct indexable_heap *dheap = &self->heap[DIRECTION_DEMOTION],
// 			      *pheap = &self->heap[DIRECTION_PROMOTION];
// 	if (self->total_samples > self->next_dump) {
// 		self->next_dump += 100000;
// 		// indexable_heap_dump(dram_heap, 1000);
// 		// indexable_heap_dump(pmem_heap, 1000);
// 	}

// 	while (true) {
// 		if (indexable_heap_size(dheap) < 512) {
// 			// Refill
// 			// pr_info("%s: ===== FINISHED: nothing to demotion =====",
// 			// 	__func__);
// 			break;
// 		}
// 		if (indexable_heap_size(pheap) < 512) {
// 			// We don't need to refill the promotion heap manually,
// 			// when we receive a sample, we will update the heap
// 			// pr_info("%s: ===== FINISHED: nothing to promotion =====",
// 			// 	__func__);
// 			break;
// 		}
// 		u64 dram_vpfn = *kv_ckey(indexable_heap_peek(dheap)),
// 		    pmem_vpfn = *kv_ckey(indexable_heap_peek(pheap));
// 		u64 dram_count = *kv_cvalue(indexable_heap_peek(dheap)),
// 		    pmem_count = *kv_cvalue(indexable_heap_peek(pheap));
// 		if (dram_count >= pmem_count) {
// 			// DRAM should contain the hottest data
// 			// pr_info("%s: ===== FINISHED: no misplaced =====",
// 			// __func__);
// 			break;
// 		}
// 		// struct folio *src __cleanup(resolve_folio_cleanup) =
// 		// 	uvirt_to_folio(mm, dram_vpfn << PAGE_SHIFT);
// 		struct folio *src =
// 			hagent_target_managed_folio(self, dram_vpfn);

// 		if (IS_ERR_OR_NULL(src) || folio_nid(src) != DRAM_NID) {
// 			// Remove invalid pfn from the heap to avoid
// 			// dead loop
// 			CLASS(hagent_target_work_time_guard, __tih)
// 			(self, EXEC_TIME_INDEXABLE_HEAP);
// 			indexable_heap_pop(dheap);
// 			continue;
// 		}

// 		// struct folio *dst __cleanup(resolve_folio_cleanup) =
// 		// 	uvirt_to_folio(mm, pmem_vpfn << PAGE_SHIFT);
// 		struct folio *dst =
// 			hagent_target_managed_folio(self, pmem_vpfn);
// 		if (IS_ERR_OR_NULL(dst) || folio_nid(dst) == DRAM_NID) {
// 			// Remove invalid pfn from the heap to avoid
// 			// dead loop
// 			CLASS(hagent_target_work_time_guard, __tih)
// 			(self, EXEC_TIME_INDEXABLE_HEAP);
// 			indexable_heap_pop(pheap);
// 			continue;
// 		}

// 		{
// 			CLASS(hagent_target_work_time_guard, __tih)
// 			(self, EXEC_TIME_INDEXABLE_HEAP);
// 			indexable_heap_pop(dheap);
// 			indexable_heap_pop(pheap);
// 		}
// 		++found_pair;

// 		// pr_info("%s: pid=%d task=%p usage=%d", __func__,
// 		// 	self->task->tgid, self->task,
// 		// 	self->task->usage);
// 		// pr_info("%s: ===== EXCHANGE EXECUTING ====", __func__);
// 		// long err = folio_exchange(src, dst, MIGRATE_SYNC);

// 		// resolve folio has taken another reference, exceeding
// 		// the expected reference count during exchange
// 		long err = ({
// 			CLASS(hagent_target_work_time_guard, __tih)
// 			(self, EXEC_TIME_EXCHANGE);
// 			folio_exchange_isolated(src, dst, MIGRATE_SYNC);
// 		});
// 		if (err) {
// 			++failed;
// 			// clang-format off
// 				pr_err("%s: exchange_folio: mode=%d err=%pe [src=%p vaddr=%p pfn=0x%lx] <-> [dst=%p vaddr=%p pfn=0x%lx]",
// 				       __func__, MIGRATE_SYNC, ERR_PTR(err),
// 				       src, (void *)(dram_vpfn << PAGE_SHIFT), folio_pfn(src),
// 				       dst, (void *)(pmem_vpfn << PAGE_SHIFT), folio_pfn(dst));
// 			// clang-format on
// 			// FIXME: should we put back the folio to the heap so that we can try at an another time?
// 		} else {
// 			CLASS(hagent_target_work_time_guard, __tih)
// 			(self, EXEC_TIME_INDEXABLE_HEAP);
// 			// Althoug in theory the folio could only be
// 			// backed by DRAM or PMEM, in practive, the
// 			// kernel could have done migration in the
// 			// background that we are not aware of.
// 			if (indexable_heap_contains(dheap, pmem_vpfn)) {
// 				indexable_heap_update(dheap, pmem_vpfn,
// 						      dram_count);
// 			} else {
// 				indexable_heap_push(dheap, pmem_vpfn,
// 						    dram_count);
// 			}
// 			if (indexable_heap_contains(pheap, dram_vpfn)) {
// 				indexable_heap_update(pheap, dram_vpfn,
// 						      pmem_count);
// 			} else {
// 				indexable_heap_push(pheap, dram_vpfn,
// 						    pmem_count);
// 			}
// 			total_exchanges++;
// 			// FIXME: implement additional vmstat counters
// 			// count_vm_event(PAGE_PROMOTED);
// 			// count_vm_event(PAGE_DEMOTED);
// 			// count_vm_event(PAGE_EXCHANGED);
// 		}
// 		// pr_info("%s: ===== EXCHANGE RETURNED ====", __func__);

// 		if (found_pair >= migration_batch_size ||
// 		    failed * 10 >= migration_batch_size) {
// 			pr_info_ratelimited("%s: found pair=%llu failed=%llu\n",
// 					    __func__, found_pair, failed);
// 			break;
// 		}
// 		// pr_info("%s: next iteration\n", __func__);
// 	}
// 	self->total_exchanges += total_exchanges;
// 	if (self->total_exchanges > self->next_report) {
// 		self->next_report += 512 * 50;
// 		pr_info("%s: total exchanges=%llu\n", __func__,
// 			self->total_exchanges);
// 	}
// 	return 0;
// }
// static int hagent_target_thread_fn_policy(struct hagent_target *self)
// {
// 	// wait on until the buffer fills up more than BUFFER_WAIT_PERCENTAGE percent
// 	TRY(mpsc_wait(self->chan), "%s: mpsc_wait()=%pe failed\n", __func__,
// 	    ERR_PTR(err));
// 	CLASS(hagent_target_work_time_guard, __t1)
// 	(self, EXEC_TIME_MIGRATION);
// 	lru_add_drain_all();
// 	struct mm_struct *mm __cleanup(cleanup_mmput) =
// 		TRY(get_task_mm(self->task), "%s: get_task_mm()=%pe failed\n",
// 		    __func__, ERR_PTR(err));
// 	// we shoule never manage a kthread which has no mm
// 	BUG_ON(!mm);
// 	scoped_cond_guard(mutex_try, ({
// 				  // count_vm_event(HAGENT_LOCK_FAILED);
// 				  count_vm_event(HAGENT_LOCK_FAILED_MIGRATION);
// 				  return -EAGAIN;
// 			  }),
// 			  &self->lock)
// 	{
// 		// count_vm_event(HAGENT_LOCK_ACQUIRED);
// 		count_vm_event(HAGENT_LOCK_ACQUIRED_MIGRATION);
// 		policy_process_samples_locked(self, mm);
// 	}
// 	return 0;
// }
// static int hagent_target_thread_fn_migration(struct hagent_target *self)
// {
// 	return 0;
// }

// #define HAGENT_TARGET_THREAD(name)                                            \
// 	static int hagent_target_thread_##name(void *p)                       \
// 	{                                                                     \
// 		struct hagent_target *self = p;                               \
// 		int err = 0;                                                  \
// 		while (!kthread_should_stop() && !err) {                      \
// 			switch (err = hagent_target_thread_fn_##name(self)) { \
// 			case -EAGAIN:                                         \
// 			case -ERESTARTSYS:                                    \
// 				err = 0;                                      \
// 				break;                                        \
// 			}                                                     \
// 		}                                                             \
// 		return err;                                                   \
// 	}

// HAGENT_TARGET_THREAD(policy);
// HAGENT_TARGET_THREAD(migration);
